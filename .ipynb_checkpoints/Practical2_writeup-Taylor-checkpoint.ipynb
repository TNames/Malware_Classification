{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TECHNICAL APPROACH\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "As each XML file contains enough information to create a nearly infinite amount of features, we had to narrow our focus. Without having much background in spyware detection, we focused on the areas suggested by the teaching staff. \n",
    "\n",
    "In the data description on the kaggle site, the course staff expressed the importance of processes and calls occuring inside the `all_section` category. Also, in the sample code, the staff provided a process to extract counts for different call types. We decided to extract features that included counts of all types of calls (inside and outside of the `all_section` category) as well as counts of first and last calls within the `all_section` category. Furthermore, the totals of calls inside and outside of `all_section` were included as features.\n",
    "\n",
    "Finally, due to a suggestion by a TF on piazza, we pursued the extraction of the bigrams of all calls within the `all_section` category. In the end, we extracted 2,669 distict features for each XML file.\n",
    "\n",
    "**GRADIENT BOOSTED TREES**\n",
    "\n",
    "Gradient boosted trees is a non-parametric technique that combines several 'weak' decision tree models trained by iteratively improving on residuals from sequentially trained trees. Gradient boosted trees are a particularly attractive method due to the ability to model non-linear behavior, feature interactions, and skewed features without transformation. Also, as an ensemble approach of many 'weak' learners, gradient boosted trees can be quite proficient in avoiding overfitting. The python package XGBoost [1] was used for the gradient boosted trees model due to its speed and parallelization.\n",
    "\n",
    "As the number of features including bigrams was quite large, hyperparameter tuning was performed on a subset of 50% of the training set. The hyperparameters tuned in a 3-fold grid search cross-validation were the following:\n",
    "\n",
    "- `'max_depth'`: maximum depth of each boosted tree [8,10,12]\n",
    "\n",
    "- `'colsample_bytree'`: Proportion of features used in each boosted tree [0.6,0.75,1]\n",
    "\n",
    "- `'subsample'`: Proportion of total samples considered for each boosted tree [0.6,0.8,1]\n",
    "\n",
    "Increasing the value of each of these hyperparameters leads to a higher variance/lower bias model. The optimal hyperparameters were selected as they provided a balanced model in variance and bias. The optimal parameters for `max_depth`, `'colsample_bytree'`, and `'subsample'` were found to be 8, 0.75 and 0.8 respectively.\n",
    "\n",
    "### RESULTS\n",
    "\n",
    "**Gradient Boosted Trees**\n",
    "\n",
    "(TO GO INSIDE TABLE WITH OTHER RESULTS)\n",
    "- XGboost 106 features: Accuracy - Test - 0.81947\n",
    "- XGBoost 2669 features: Accuracy - Test - 0.82947\n",
    "- XGBoost 2669 features (CV): Accuracy - Test - 0.82895\n",
    "\n",
    "### DISCUSSION\n",
    "\n",
    "**Gradient Boosted Trees**\n",
    "\n",
    "Analysis using XGBoost was broken down into three iterations. First, the model was trained utilizing the original \"count\" features without the use of bigrams. Hyperparameters were chosen through a 3-fold cross validation procedure. The result was very positive, as with a mere 106 features, we were achieving around 82% accuracy. The second iteration included all occuring bigrams in the training set. Initially, trained our model using the hyperparameters from the previous iteration. Once again, we saw a significant improvment with an accuracy of nearly 83%. Finally, we performed hyperparameter tuning through cross validation and the result was nearly identical. \n",
    "\n",
    "During this process, we noticed a discrepancy between the cross-validation score (which hovered around 90% accuracy) and the score on the public leaderboard (83% accuracy).  Despite the cross validation procedure, our model was overfitting. Possible explanations for this overfitting could be due to the small training set size compared to the test set, or an unbalanced class proportions between the training and test sets. \n",
    "\n",
    "Finally, an analysis of the confusion matrix of our final model shed some light on where our model was misclassifying. The confusion matrix is shown in the following figure. The model was misclassifying a significant proportion of malware belonging to the Zbot, Virut, and Agent classes as \"None\". To improve the classification accuracy, further analysis could be completed to help distinguish between the \"None\" class from the previously mentioned classes. \n",
    "\n",
    "**(INSERT ATTACHED FIGURE HERE DAVID)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### REFERENCES\n",
    "\n",
    "\n",
    "[1] XGBoost - https://github.com/dmlc/xgboost\n",
    "\n",
    "\n",
    "### APPENDICES\n",
    "\n",
    "All code from this practical can be found at the following Google Drive link:\n",
    "\n",
    "https://drive.google.com/folderview?id=0B0e1_K8CvqynSWJuenRUdFRJN1U&usp=sharing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
