{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS181 PRACTICAL 2 - Malware Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING WITH ORIGINAL FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, counts of calls within the `all_section` branch and outside were completed. Also, calls which were first or last within the `all_section` branch were counted. These basic features were extracted using the scripts `basic_feat_ext_train.py` and `basic_feat_ext_test.py` included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"train_feat.csv\", index_col=0)\n",
    "df.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_feat=[]\n",
    "\n",
    "#remove columns composed of zeros\n",
    "for i in df.columns:\n",
    "    if df[i].mean() != 0:\n",
    "        good_feat.append(i)\n",
    "df=df[good_feat]\n",
    "features = list(df.columns)\n",
    "features.remove('response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#observe unique class values\n",
    "set(df['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORM CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an initial idea for hyperparameter ranges and expected classification accuracy, educated guesses for the hyperparameters were used on the following cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-merror-mean</th>\n",
       "      <th>test-merror-std</th>\n",
       "      <th>train-merror-mean</th>\n",
       "      <th>train-merror-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148784</td>\n",
       "      <td>0.012016</td>\n",
       "      <td>0.131848</td>\n",
       "      <td>0.002011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.143274</td>\n",
       "      <td>0.015051</td>\n",
       "      <td>0.124473</td>\n",
       "      <td>0.003582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142626</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.122366</td>\n",
       "      <td>0.002784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.141977</td>\n",
       "      <td>0.014409</td>\n",
       "      <td>0.119854</td>\n",
       "      <td>0.003839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.015355</td>\n",
       "      <td>0.117828</td>\n",
       "      <td>0.003266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.137763</td>\n",
       "      <td>0.013167</td>\n",
       "      <td>0.116937</td>\n",
       "      <td>0.003028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.141005</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>0.116127</td>\n",
       "      <td>0.004231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.137763</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.113533</td>\n",
       "      <td>0.003932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.138088</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.113371</td>\n",
       "      <td>0.003461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.138412</td>\n",
       "      <td>0.013151</td>\n",
       "      <td>0.112318</td>\n",
       "      <td>0.003256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.137115</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.111021</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.138087</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>0.110697</td>\n",
       "      <td>0.002847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.136791</td>\n",
       "      <td>0.012663</td>\n",
       "      <td>0.109806</td>\n",
       "      <td>0.002601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.134198</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.109076</td>\n",
       "      <td>0.001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.133873</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.109319</td>\n",
       "      <td>0.001907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.132253</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.002396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.133873</td>\n",
       "      <td>0.009864</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.131929</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.105672</td>\n",
       "      <td>0.002542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.130956</td>\n",
       "      <td>0.011012</td>\n",
       "      <td>0.105267</td>\n",
       "      <td>0.002040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.130308</td>\n",
       "      <td>0.011303</td>\n",
       "      <td>0.105349</td>\n",
       "      <td>0.002018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.130632</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>0.104862</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.131280</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>0.103566</td>\n",
       "      <td>0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.131605</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.103241</td>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.131280</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>0.102350</td>\n",
       "      <td>0.002357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.131604</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.102188</td>\n",
       "      <td>0.002210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.132901</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.101945</td>\n",
       "      <td>0.002184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.131280</td>\n",
       "      <td>0.009225</td>\n",
       "      <td>0.100972</td>\n",
       "      <td>0.002450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.130956</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>0.100567</td>\n",
       "      <td>0.002788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.099676</td>\n",
       "      <td>0.002651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.128363</td>\n",
       "      <td>0.011248</td>\n",
       "      <td>0.099189</td>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.108266</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>0.035008</td>\n",
       "      <td>0.001245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.108266</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>0.035089</td>\n",
       "      <td>0.001371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.107942</td>\n",
       "      <td>0.011579</td>\n",
       "      <td>0.035008</td>\n",
       "      <td>0.001245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.107617</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>0.034846</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.034765</td>\n",
       "      <td>0.001266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.034846</td>\n",
       "      <td>0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.034684</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.107617</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.034441</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.107617</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.034360</td>\n",
       "      <td>0.000938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.107617</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>0.034360</td>\n",
       "      <td>0.000938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.107617</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.034360</td>\n",
       "      <td>0.000938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.107942</td>\n",
       "      <td>0.011579</td>\n",
       "      <td>0.034198</td>\n",
       "      <td>0.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.034279</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.034117</td>\n",
       "      <td>0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.107617</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.034198</td>\n",
       "      <td>0.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.034198</td>\n",
       "      <td>0.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.034036</td>\n",
       "      <td>0.000924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.001038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033874</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033874</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.107617</td>\n",
       "      <td>0.011533</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033711</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.033630</td>\n",
       "      <td>0.000810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>0.033549</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.106969</td>\n",
       "      <td>0.011322</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011479</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.011479</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-merror-mean  test-merror-std  train-merror-mean  train-merror-std\n",
       "0            0.148784         0.012016           0.131848          0.002011\n",
       "1            0.143274         0.015051           0.124473          0.003582\n",
       "2            0.142626         0.013247           0.122366          0.002784\n",
       "3            0.141977         0.014409           0.119854          0.003839\n",
       "4            0.140681         0.015355           0.117828          0.003266\n",
       "5            0.137763         0.013167           0.116937          0.003028\n",
       "6            0.141005         0.012129           0.116127          0.004231\n",
       "7            0.137763         0.010043           0.113533          0.003932\n",
       "8            0.138088         0.008353           0.113371          0.003461\n",
       "9            0.138412         0.013151           0.112318          0.003256\n",
       "10           0.137115         0.012024           0.111021          0.003064\n",
       "11           0.138087         0.012445           0.110697          0.002847\n",
       "12           0.136791         0.012663           0.109806          0.002601\n",
       "13           0.134198         0.014255           0.109076          0.001608\n",
       "14           0.133873         0.012495           0.109319          0.001907\n",
       "15           0.132253         0.010925           0.107293          0.002396\n",
       "16           0.133873         0.009864           0.106321          0.001711\n",
       "17           0.131929         0.010877           0.105672          0.002542\n",
       "18           0.130956         0.011012           0.105267          0.002040\n",
       "19           0.130308         0.011303           0.105349          0.002018\n",
       "20           0.130632         0.011350           0.104862          0.001783\n",
       "21           0.131280         0.011322           0.103566          0.001876\n",
       "22           0.131605         0.011154           0.103241          0.002326\n",
       "23           0.131280         0.009451           0.102350          0.002357\n",
       "24           0.131604         0.009248           0.102188          0.002210\n",
       "25           0.132901         0.008453           0.101945          0.002184\n",
       "26           0.131280         0.009225           0.100972          0.002450\n",
       "27           0.130956         0.008959           0.100567          0.002788\n",
       "28           0.129336         0.011059           0.099676          0.002651\n",
       "29           0.128363         0.011248           0.099189          0.001975\n",
       "..                ...              ...                ...               ...\n",
       "470          0.108266         0.011705           0.035008          0.001245\n",
       "471          0.108266         0.011705           0.035089          0.001371\n",
       "472          0.107942         0.011579           0.035008          0.001245\n",
       "473          0.107617         0.011892           0.034846          0.001174\n",
       "474          0.107293         0.012275           0.034765          0.001266\n",
       "475          0.107293         0.012275           0.034846          0.001229\n",
       "476          0.107293         0.012275           0.034684          0.001191\n",
       "477          0.107617         0.011980           0.034441          0.001025\n",
       "478          0.107617         0.011980           0.034360          0.000938\n",
       "479          0.107617         0.011533           0.034360          0.000938\n",
       "480          0.107617         0.011980           0.034360          0.000938\n",
       "481          0.107942         0.011579           0.034198          0.000910\n",
       "482          0.107293         0.012275           0.034279          0.001012\n",
       "483          0.107293         0.012275           0.034117          0.000826\n",
       "484          0.107617         0.011980           0.034198          0.000910\n",
       "485          0.107293         0.011928           0.034198          0.000910\n",
       "486          0.107293         0.011928           0.034036          0.000924\n",
       "487          0.107293         0.011928           0.033955          0.001038\n",
       "488          0.107293         0.011928           0.033955          0.000826\n",
       "489          0.107293         0.011928           0.033874          0.000752\n",
       "490          0.107293         0.011928           0.033874          0.000752\n",
       "491          0.107293         0.011928           0.033792          0.000752\n",
       "492          0.107293         0.011928           0.033792          0.000752\n",
       "493          0.107617         0.011533           0.033792          0.000752\n",
       "494          0.107293         0.011928           0.033711          0.000865\n",
       "495          0.107293         0.011928           0.033630          0.000810\n",
       "496          0.107293         0.011011           0.033549          0.000865\n",
       "497          0.106969         0.011322           0.033387          0.000658\n",
       "498          0.107293         0.011479           0.033387          0.000658\n",
       "499          0.107293         0.011479           0.033387          0.000658\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(df[features], df['response']) \n",
    "paramcv =     {\n",
    "    'booster' : \"gbtree\", \n",
    "    'silent': 0 , \n",
    "    'eta'  :0.01, \n",
    "    'max_depth' :7, \n",
    "    'min_child_weight':3,  \n",
    "    'subsample' :0.85, \n",
    "    'colsample_bytree' :0.66, \n",
    "\n",
    "    'objective': \"multi:softmax\",        \n",
    "    'eval_metric' : 'merror', \n",
    "    'num_class' : 15,\n",
    "    'seed':500 \n",
    "    }\n",
    "\n",
    "test=xgb.cv(paramcv, dtrain, 500, nfold=5, metrics={'merror'}, seed = 0)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to obvious overfitting of the previous parameters, we perform a more thorough grid cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST {'subsample': 0.8, 'colsample_bytree': 1, 'max_depth': 10} 0.887556707712 [mean: 0.87946, std: 0.00556, params: {'subsample': 0.5, 'colsample_bytree': 0.5, 'max_depth': 6}, mean: 0.88172, std: 0.00314, params: {'subsample': 0.8, 'colsample_bytree': 0.5, 'max_depth': 6}, mean: 0.88334, std: 0.00391, params: {'subsample': 1, 'colsample_bytree': 0.5, 'max_depth': 6}, mean: 0.87913, std: 0.00603, params: {'subsample': 0.5, 'colsample_bytree': 0.5, 'max_depth': 8}, mean: 0.88399, std: 0.00313, params: {'subsample': 0.8, 'colsample_bytree': 0.5, 'max_depth': 8}, mean: 0.88399, std: 0.00189, params: {'subsample': 1, 'colsample_bytree': 0.5, 'max_depth': 8}, mean: 0.87913, std: 0.00474, params: {'subsample': 0.5, 'colsample_bytree': 0.5, 'max_depth': 10}, mean: 0.88464, std: 0.00348, params: {'subsample': 0.8, 'colsample_bytree': 0.5, 'max_depth': 10}, mean: 0.88594, std: 0.00253, params: {'subsample': 1, 'colsample_bytree': 0.5, 'max_depth': 10}, mean: 0.88172, std: 0.00461, params: {'subsample': 0.5, 'colsample_bytree': 0.75, 'max_depth': 6}, mean: 0.88399, std: 0.00622, params: {'subsample': 0.8, 'colsample_bytree': 0.75, 'max_depth': 6}, mean: 0.88302, std: 0.00362, params: {'subsample': 1, 'colsample_bytree': 0.75, 'max_depth': 6}, mean: 0.88140, std: 0.00499, params: {'subsample': 0.5, 'colsample_bytree': 0.75, 'max_depth': 8}, mean: 0.88464, std: 0.00564, params: {'subsample': 0.8, 'colsample_bytree': 0.75, 'max_depth': 8}, mean: 0.88432, std: 0.00257, params: {'subsample': 1, 'colsample_bytree': 0.75, 'max_depth': 8}, mean: 0.88010, std: 0.00461, params: {'subsample': 0.5, 'colsample_bytree': 0.75, 'max_depth': 10}, mean: 0.88432, std: 0.00555, params: {'subsample': 0.8, 'colsample_bytree': 0.75, 'max_depth': 10}, mean: 0.88529, std: 0.00506, params: {'subsample': 1, 'colsample_bytree': 0.75, 'max_depth': 10}, mean: 0.88237, std: 0.00658, params: {'subsample': 0.5, 'colsample_bytree': 1, 'max_depth': 6}, mean: 0.88302, std: 0.00500, params: {'subsample': 0.8, 'colsample_bytree': 1, 'max_depth': 6}, mean: 0.88108, std: 0.00387, params: {'subsample': 1, 'colsample_bytree': 1, 'max_depth': 6}, mean: 0.88367, std: 0.00359, params: {'subsample': 0.5, 'colsample_bytree': 1, 'max_depth': 8}, mean: 0.88432, std: 0.00325, params: {'subsample': 0.8, 'colsample_bytree': 1, 'max_depth': 8}, mean: 0.88496, std: 0.00357, params: {'subsample': 1, 'colsample_bytree': 1, 'max_depth': 8}, mean: 0.88367, std: 0.00291, params: {'subsample': 0.5, 'colsample_bytree': 1, 'max_depth': 10}, mean: 0.88756, std: 0.00460, params: {'subsample': 0.8, 'colsample_bytree': 1, 'max_depth': 10}, mean: 0.88691, std: 0.00542, params: {'subsample': 1, 'colsample_bytree': 1, 'max_depth': 10}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "params = {\"max_depth\": [6, 8, 10],\n",
    "          'colsample_bytree':[0.5,0.75,1],\n",
    "          'subsample':[0.5,0.8,1]}\n",
    "\n",
    "gbrt = xgb.XGBClassifier(nthread=-1, objective=\"multi:softmax\", \n",
    "                         learning_rate=0.05, n_estimators=20, silent=0, seed=50)\n",
    "\n",
    "\n",
    "cv = GridSearchCV(gbrt, param_grid=params, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "cv.fit(df[features].values, df['response'].values)\n",
    "print \"BEST\", cv.best_params_, cv.best_score_, cv.grid_scores_\n",
    "best = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use best parameters to train on entire dataset and make first predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.02, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=500, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=50, silent=0, subsample=0.8)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt = xgb.XGBClassifier(nthread=-1, objective=\"multi:softmax\", \n",
    "                         max_depth = 10, colsample_bytree = 1, subsample = 0.8,\n",
    "                         learning_rate=0.02, n_estimators=500, silent=0, seed=50)\n",
    "gbrt.fit(df[features].values, df['response'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"test_feat.csv\", index_col=0)\n",
    "df_test.set_index(\"id\", inplace=True)\n",
    "\n",
    "y_hat=gbrt.predict(df_test[features].values)\n",
    "submit=pd.DataFrame(df_test.index)\n",
    "submit['Prediction']=y_hat\n",
    "submit.set_index('id', inplace=True)\n",
    "submit.to_csv('gbrt_no_ngrams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INCLUDING BIGRAM FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, all possible bigrams within the `all-section` tag for all training files were saved to a text file. Each of these bigrams (and their counts) will represent a feature in the final design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = set([])\n",
    "\n",
    "for datafile in os.listdir('train'):\n",
    "    tree = ET.parse(os.path.join('train',datafile))\n",
    "    last=False\n",
    "    for el in tree.iter():\n",
    "        if el.tag == \"all_section\":\n",
    "            for child in el:\n",
    "                if last:\n",
    "                    bigram=last+\"_\"+child.tag\n",
    "                    bigrams.add(bigram)\n",
    "                last= child.tag\n",
    "\n",
    "np.savetxt('bigrams.txt', np.array(list(bigrams)), fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bigrams text file was utilized in the feature extraction script for the bigrams (`bigrams_test.py` and `bigrams_train.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate old features with new bigram features for test set, rename columns\n",
    "df_bigrams=pd.read_csv(\"train_bigram_feat.csv\", index_col=0)\n",
    "df_bigrams.set_index('id', inplace=True)\n",
    "df_bigrams.drop('response', axis=1, inplace=True)\n",
    "df_train = pd.concat([df, df_bigrams], axis=1, join_axes=[df.index])\n",
    "\n",
    "cols=[]\n",
    "for i, name in enumerate(df_train.columns):\n",
    "    if name =='response':\n",
    "        cols.append('response')\n",
    "    else:\n",
    "        cols.append(str(i))\n",
    "\n",
    "df_train.columns=cols\n",
    "df_train.to_csv('bigram_train.csv')\n",
    "\n",
    "#create feature name list\n",
    "features=cols[:]\n",
    "features.remove('response')\n",
    "\n",
    "# Concatenate old features with new bigram features for test set, rename columns\n",
    "df_test=df_test[good_feat]\n",
    "df_bigrams_test=pd.read_csv(\"test_bigram_feat.csv\", index_col=0)\n",
    "df_bigrams_test.set_index('id', inplace=True)\n",
    "df_bigrams_test.drop('response', axis=1, inplace=True)\n",
    "df_test_bi = pd.concat([df_test, df_bigrams_test], axis=1, join_axes=[df_test.index])\n",
    "df_test_bi.columns=cols\n",
    "df_test_bi.to_csv('bigram_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORM CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-merror-mean</th>\n",
       "      <th>test-merror-std</th>\n",
       "      <th>train-merror-mean</th>\n",
       "      <th>train-merror-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.147536</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>0.121433</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139429</td>\n",
       "      <td>0.004375</td>\n",
       "      <td>0.111705</td>\n",
       "      <td>0.003898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.137159</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>0.106842</td>\n",
       "      <td>0.002644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137484</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.109598</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134565</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>0.105707</td>\n",
       "      <td>0.002981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.135538</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.105869</td>\n",
       "      <td>0.002427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.132296</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>0.102951</td>\n",
       "      <td>0.002584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132944</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.101816</td>\n",
       "      <td>0.002927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.133593</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.102140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.134565</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.100357</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.132620</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>0.100519</td>\n",
       "      <td>0.000827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.134565</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.099708</td>\n",
       "      <td>0.001731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.133917</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.098898</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.134890</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>0.099222</td>\n",
       "      <td>0.002998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.134565</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.099222</td>\n",
       "      <td>0.003152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.134241</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.098411</td>\n",
       "      <td>0.003424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.133593</td>\n",
       "      <td>0.006169</td>\n",
       "      <td>0.099870</td>\n",
       "      <td>0.003307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.133593</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.098573</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.132944</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.097925</td>\n",
       "      <td>0.005122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.133269</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.098087</td>\n",
       "      <td>0.005106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.132944</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.097438</td>\n",
       "      <td>0.005406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.131972</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>0.004966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.130999</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.095006</td>\n",
       "      <td>0.005787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.131323</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.094844</td>\n",
       "      <td>0.005372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.130674</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.093710</td>\n",
       "      <td>0.005363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.130350</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.094196</td>\n",
       "      <td>0.004853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.129702</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>0.092737</td>\n",
       "      <td>0.005168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.129702</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.091602</td>\n",
       "      <td>0.006371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.128080</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.090629</td>\n",
       "      <td>0.006553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.128081</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.090467</td>\n",
       "      <td>0.006392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.105707</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.024643</td>\n",
       "      <td>0.002187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.106031</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.024643</td>\n",
       "      <td>0.002187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.105707</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.024643</td>\n",
       "      <td>0.002187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.106031</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.106031</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.106031</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.106031</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.024481</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.106031</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.024319</td>\n",
       "      <td>0.002211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.106031</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.024319</td>\n",
       "      <td>0.002211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.105707</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.024157</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023995</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023833</td>\n",
       "      <td>0.002211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.104734</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-merror-mean  test-merror-std  train-merror-mean  train-merror-std\n",
       "0            0.147536         0.008927           0.121433          0.001791\n",
       "1            0.139429         0.004375           0.111705          0.003898\n",
       "2            0.137159         0.006304           0.106842          0.002644\n",
       "3            0.137484         0.006755           0.109598          0.000999\n",
       "4            0.134565         0.006848           0.105707          0.002981\n",
       "5            0.135538         0.008749           0.105869          0.002427\n",
       "6            0.132296         0.007660           0.102951          0.002584\n",
       "7            0.132944         0.008343           0.101816          0.002927\n",
       "8            0.133593         0.003997           0.102140          0.000000\n",
       "9            0.134565         0.005635           0.100357          0.000999\n",
       "10           0.132620         0.005962           0.100519          0.000827\n",
       "11           0.134565         0.006014           0.099708          0.001731\n",
       "12           0.133917         0.006469           0.098898          0.001999\n",
       "13           0.134890         0.005106           0.099222          0.002998\n",
       "14           0.134565         0.004853           0.099222          0.003152\n",
       "15           0.134241         0.004765           0.098411          0.003424\n",
       "16           0.133593         0.006169           0.099870          0.003307\n",
       "17           0.133593         0.003307           0.098573          0.004788\n",
       "18           0.132944         0.003210           0.097925          0.005122\n",
       "19           0.133269         0.004422           0.098087          0.005106\n",
       "20           0.132944         0.003997           0.097438          0.005406\n",
       "21           0.131972         0.003582           0.097439          0.004966\n",
       "22           0.130999         0.003582           0.095006          0.005787\n",
       "23           0.131323         0.004766           0.094844          0.005372\n",
       "24           0.130674         0.004853           0.093710          0.005363\n",
       "25           0.130350         0.005208           0.094196          0.004853\n",
       "26           0.129702         0.005962           0.092737          0.005168\n",
       "27           0.129702         0.005635           0.091602          0.006371\n",
       "28           0.128080         0.006066           0.090629          0.006553\n",
       "29           0.128081         0.005044           0.090467          0.006392\n",
       "..                ...              ...                ...               ...\n",
       "470          0.105707         0.004853           0.024643          0.002187\n",
       "471          0.106031         0.004422           0.024643          0.002187\n",
       "472          0.105707         0.004853           0.024643          0.002187\n",
       "473          0.106031         0.005208           0.024481          0.002394\n",
       "474          0.106031         0.005208           0.024481          0.002394\n",
       "475          0.106031         0.005208           0.024481          0.002394\n",
       "476          0.106031         0.005208           0.024481          0.002394\n",
       "477          0.106031         0.005208           0.024319          0.002211\n",
       "478          0.106031         0.005208           0.024319          0.002211\n",
       "479          0.105707         0.005635           0.024157          0.002038\n",
       "480          0.105058         0.004960           0.024157          0.002038\n",
       "481          0.105058         0.004960           0.024157          0.002038\n",
       "482          0.105058         0.004960           0.024157          0.002038\n",
       "483          0.105058         0.004960           0.024157          0.002038\n",
       "484          0.105058         0.004960           0.024157          0.002038\n",
       "485          0.105058         0.005728           0.024157          0.002038\n",
       "486          0.104734         0.005406           0.024157          0.002038\n",
       "487          0.104734         0.005406           0.024157          0.002038\n",
       "488          0.105058         0.004960           0.024157          0.002038\n",
       "489          0.104734         0.005406           0.024157          0.002038\n",
       "490          0.104734         0.005406           0.024157          0.002038\n",
       "491          0.104734         0.005406           0.023995          0.001999\n",
       "492          0.104734         0.005406           0.023833          0.002211\n",
       "493          0.104734         0.005406           0.023670          0.002038\n",
       "494          0.104734         0.005406           0.023670          0.002038\n",
       "495          0.104734         0.005406           0.023670          0.002038\n",
       "496          0.104734         0.005406           0.023670          0.002038\n",
       "497          0.104734         0.005406           0.023670          0.002038\n",
       "498          0.104734         0.005406           0.023670          0.002038\n",
       "499          0.104734         0.005406           0.023670          0.002038\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(df_train[features], df_train['response']) \n",
    "paramcv =     {     \n",
    "    'booster' : \"gbtree\", \n",
    "    'silent': 0 , \n",
    "    'eta'  :0.01, \n",
    "    'max_depth'           :10, \n",
    "    'min_child_weight':3,  \n",
    "    'subsample'           :0.85, \n",
    "    'colsample_bytree'    :0.66, \n",
    "    'objective': \"multi:softmax\",\n",
    "    'eval_metric' : 'merror',\n",
    "    'num_class' : 15,\n",
    "    'seed':500\n",
    "  \n",
    "    }\n",
    "\n",
    "test=xgb.cv(paramcv, dtrain, 500, nfold=3, metrics={'merror'}, seed = 0)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using same hyperparameters from previous cross validation, we fit the model using 1000 boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=50, silent=0, subsample=0.8)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_bigram = xgb.XGBClassifier(nthread=-1, objective=\"multi:softmax\", \n",
    "                         max_depth = 10, colsample_bytree = 1, subsample = 0.8,\n",
    "                         learning_rate=0.01, n_estimators=1000, silent=0, seed=50)\n",
    "gbt_bigram.fit(df_train[features].values, df_train['response'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat=gbt_bigram.predict(df_test_bi[features].values)\n",
    "submit=pd.DataFrame(df_test_bi.index)\n",
    "submit['Prediction']=y_hat\n",
    "submit.set_index('id', inplace=True)\n",
    "submit.to_csv('gbt_bigrams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many features were added (with the addition of bigrams), perhaps the previous hyperparameters are no longer the best. Here we perform a more in depth cross-validation on a subsample of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST {'subsample': 0.8, 'colsample_bytree': 0.75, 'max_depth': 8} 0.896953985742 [mean: 0.89177, std: 0.00458, params: {'subsample': 0.6, 'colsample_bytree': 0.6, 'max_depth': 8}, mean: 0.89501, std: 0.00272, params: {'subsample': 0.8, 'colsample_bytree': 0.6, 'max_depth': 8}, mean: 0.89566, std: 0.00300, params: {'subsample': 1, 'colsample_bytree': 0.6, 'max_depth': 8}, mean: 0.89177, std: 0.00452, params: {'subsample': 0.6, 'colsample_bytree': 0.6, 'max_depth': 10}, mean: 0.89436, std: 0.00220, params: {'subsample': 0.8, 'colsample_bytree': 0.6, 'max_depth': 10}, mean: 0.89242, std: 0.00332, params: {'subsample': 1, 'colsample_bytree': 0.6, 'max_depth': 10}, mean: 0.89112, std: 0.00543, params: {'subsample': 0.6, 'colsample_bytree': 0.6, 'max_depth': 12}, mean: 0.89307, std: 0.00220, params: {'subsample': 0.8, 'colsample_bytree': 0.6, 'max_depth': 12}, mean: 0.89371, std: 0.00380, params: {'subsample': 1, 'colsample_bytree': 0.6, 'max_depth': 12}, mean: 0.89371, std: 0.00200, params: {'subsample': 0.6, 'colsample_bytree': 0.75, 'max_depth': 8}, mean: 0.89695, std: 0.00191, params: {'subsample': 0.8, 'colsample_bytree': 0.75, 'max_depth': 8}, mean: 0.89307, std: 0.00188, params: {'subsample': 1, 'colsample_bytree': 0.75, 'max_depth': 8}, mean: 0.89242, std: 0.00362, params: {'subsample': 0.6, 'colsample_bytree': 0.75, 'max_depth': 10}, mean: 0.89566, std: 0.00184, params: {'subsample': 0.8, 'colsample_bytree': 0.75, 'max_depth': 10}, mean: 0.89307, std: 0.00216, params: {'subsample': 1, 'colsample_bytree': 0.75, 'max_depth': 10}, mean: 0.89112, std: 0.00403, params: {'subsample': 0.6, 'colsample_bytree': 0.75, 'max_depth': 12}, mean: 0.89436, std: 0.00101, params: {'subsample': 0.8, 'colsample_bytree': 0.75, 'max_depth': 12}, mean: 0.89242, std: 0.00146, params: {'subsample': 1, 'colsample_bytree': 0.75, 'max_depth': 12}, mean: 0.89566, std: 0.00184, params: {'subsample': 0.6, 'colsample_bytree': 1, 'max_depth': 8}, mean: 0.89371, std: 0.00416, params: {'subsample': 0.8, 'colsample_bytree': 1, 'max_depth': 8}, mean: 0.89177, std: 0.00043, params: {'subsample': 1, 'colsample_bytree': 1, 'max_depth': 8}, mean: 0.89501, std: 0.00345, params: {'subsample': 0.6, 'colsample_bytree': 1, 'max_depth': 10}, mean: 0.89436, std: 0.00371, params: {'subsample': 0.8, 'colsample_bytree': 1, 'max_depth': 10}, mean: 0.89371, std: 0.00240, params: {'subsample': 1, 'colsample_bytree': 1, 'max_depth': 10}, mean: 0.89371, std: 0.00341, params: {'subsample': 0.6, 'colsample_bytree': 1, 'max_depth': 12}, mean: 0.89566, std: 0.00144, params: {'subsample': 0.8, 'colsample_bytree': 1, 'max_depth': 12}, mean: 0.89371, std: 0.00274, params: {'subsample': 1, 'colsample_bytree': 1, 'max_depth': 12}]\n"
     ]
    }
   ],
   "source": [
    "#reduce size of training set for faster cross validation\n",
    "small = df_train.sample(frac=0.5)   \n",
    "params = {\"max_depth\": [8, 10, 12],\n",
    "          'colsample_bytree':[0.6,0.75,1],\n",
    "          'subsample':[0.6,0.8,1]}\n",
    "\n",
    "cv_gbt = xgb.XGBClassifier(nthread=-1, objective=\"multi:softmax\", \n",
    "                         learning_rate=0.02, n_estimators=300, silent=0, seed=50)\n",
    "\n",
    "\n",
    "cv = GridSearchCV(cv_gbt, param_grid=params, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "cv.fit(small[features].values, small['response'].values)\n",
    "print \"BEST\", cv.best_params_, cv.best_score_, cv.grid_scores_\n",
    "best = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best performing hyperparameters, we train using all training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbt_bigram = xgb.XGBClassifier(nthread=-1, objective=\"multi:softmax\", \n",
    "                         max_depth = 8, colsample_bytree = 0.75, subsample = 0.8,\n",
    "                         learning_rate=0.01, n_estimators=1000, silent=0, seed=50)\n",
    "gbt_bigram.fit(df_train[features].values, df_train['response'].values)\n",
    "y_hat=gbt_bigram.predict(df_test_bi[features].values)\n",
    "submit=pd.DataFrame(df_test_bi.index)\n",
    "submit['Prediction']=y_hat\n",
    "submit.set_index('id', inplace=True)\n",
    "submit.to_csv('gbt_1000_bigrams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFUSION MATRIX ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand which classes the model is misclassifying, we perform an analysis of the confusion matrix using predictions on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "malware_classes = [\"Agent\", \"AutoRun\", \"FraudLoad\", \"FraudPack\", \"Hupigon\", \"Krap\",\n",
    "           \"Lipler\", \"Magania\", \"None\", \"Poison\", \"Swizzor\", \"Tdss\",\n",
    "           \"VB\", \"Virut\", \"Zbot\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[features].values,\n",
    "                                                    df_train['response'].values, \n",
    "                                                    test_size=0.25,random_state=0)\n",
    "\n",
    "\n",
    "gbt_val_bigram = xgb.XGBClassifier(nthread=-1, objective=\"multi:softmax\", \n",
    "                         max_depth = 8, colsample_bytree = 0.75, subsample = 0.8,\n",
    "                         learning_rate=0.01, n_estimators=500, silent=0, seed=50)\n",
    "y_pred = gbt_val_bigram.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110c93410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAG5CAYAAABm2faLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmqh+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119cc9dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code based on link:\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(malware_classes))\n",
    "    plt.xticks(tick_marks, malware_classes, rotation=45)\n",
    "    plt.yticks(tick_marks, malware_classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label');\n",
    "    plt.xlabel('Predicted label');\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure();\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in an attempt to improve upon our best model's accuracy, we perform an ensemble approach combining previous predictions performed above. The goal of the first round of ensembling is to create predictions that limit the number of 'None\" predictions. The three models used were high performing models, but do not include the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred1=pd.read_csv(\"gbt_sub_500_bigrams.csv\", index_col=0)\n",
    "pred2=pd.read_csv(\"gbt_1000_bigrams.csv\", index_col=0)\n",
    "pred3=pd.read_csv(\"gbt_no_ngrams.csv\", index_col=0)\n",
    "pred1[\"pred2\"]=pred2.Prediction\n",
    "pred1[\"pred3\"]=pred3.Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a slice of a dataframe where the main model (pred2) predicts 'None', while one of the two others does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small = pred1[(pred2.Prediction==8)]\n",
    "rm = small[(small.Prediction!=small.pred2) | (small.pred2!=small.pred3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for the often confused classes (stored in list `switch`), if one of the predictions of the two secondary models is one of the `switch` classes, the prediction is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "switch = [0,4,9,10,11,13,14]\n",
    "new=[]\n",
    "for i in rm.iterrows(): \n",
    "    if i[1][0] in switch:\n",
    "        new.append(i[1][0])\n",
    "    elif i[1][2] in switch:\n",
    "        new.append(i[1][2])\n",
    "    else:\n",
    "        new.append(i[1][1])\n",
    "\n",
    "rm['pred4'] = new\n",
    "pred2.loc[rm.index,'Prediction'] = new\n",
    "\n",
    "pred2.to_csv('none_hacking.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling Round 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two final ensemble models are created by combining the best performing model `gbt_bigrams` with another high performing model and the previously created predictions limiting 'Nones'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hack1=pd.read_csv(\"none_hacking.csv\", index_col=0)\n",
    "hack2=pd.read_csv(\"gbt_bigrams.csv\", index_col=0)\n",
    "hack3=pd.read_csv(\"gbt_300_bigrams.csv\", index_col=0)\n",
    "hack1['pred2']=hack2.Prediction\n",
    "hack1['pred3']=hack3.Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two predictions are made:\n",
    "- If the two secondary models have the same prediction, use this prediction. If not, use primary.\n",
    "- Same as above, but if two secondary models make prediction of 'None' (`8`), don't replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new1=[]\n",
    "changes1=0\n",
    "for i in hack1.iterrows():\n",
    "    if (i[1][1] == i[1][2]):\n",
    "        if (i[1][1] != i[1][0]):\n",
    "            changes1+=1\n",
    "        new1.append(i[1][1])\n",
    "    else:\n",
    "        new1.append(i[1][0])\n",
    "        \n",
    "new2=[]\n",
    "changes2=0\n",
    "for i in hack1.iterrows():\n",
    "    if (i[1][1] == i[1][2]) & (i[1][1]!=8):\n",
    "        if (i[1][1] != i[1][0]):\n",
    "            changes2+=1\n",
    "        new2.append(i[1][1])\n",
    "    else:\n",
    "        new2.append(i[1][0])\n",
    "\n",
    "pd.DataFrame(data ={'Prediction': new1} , index = hack1.index).to_csv('voting_w_8s.csv')\n",
    "pd.DataFrame(data ={'Prediction': new2} , index = hack1.index).to_csv('voting_wo_8s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the predictions from the majority vote with no elimination of 'Nones' had the highest accuracy score.\n",
    "\n",
    "FINAL RESULT: **3rd PLACE** of 80 TEAMS\n",
    "\n",
    "https://inclass.kaggle.com/c/cs181-s16-classifying-malicious-software/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
